# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_mlp.ipynb.

# %% auto 0
__all__ = ['Perceptron']

# %% ../nbs/01_mlp.ipynb 3
from fastcore.utils import *
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# %% ../nbs/01_mlp.ipynb 4
class Perceptron:
    "A perceptron model"

    def __init__(
        self,
        no_of_inputs: int,
        no_of_iterations: int = 1000,
        learning_rate: float = 0.1,
    ) -> None:
        self.no_of_inputs = no_of_inputs
        self.no_of_iterations = no_of_iterations
        self.learning_rate = learning_rate
        self.weights = np.zeros(
            self.no_of_inputs + 1
        )  ## TODO change to random values close to 0

    def __str__(self) -> str:
        "When printing show current `weights`"
        return f"{self.weights}"

    __repr__ = __str__  # Same jupyter representation as print

# %% ../nbs/01_mlp.ipynb 12
@patch
def activation_function(self: Perceptron, x: float):  # Weighted input to neuron
    "Temporary inner basic activation function -- positivity check"
    if x >= 0:
        return 1
    else:
        return 0

# %% ../nbs/01_mlp.ipynb 13
@patch
def output(self: Perceptron, x):  # Values in columns from input data
    "Output from neuron `f(x^Tw)`"
    return self.activation_function(np.dot(self.weights[1:], x) + self.weights[0])

# %% ../nbs/01_mlp.ipynb 14
@patch
def train(self: Perceptron, data_x, data_y):  # Input data columns  # Correct labels
    "Training loop repeted `no_of_iterations` times"
    for _ in range(self.no_of_iterations):
        # ZAD. DOM losowanie przykładów
        for x, y in zip(data_x, data_y):
            prediction = self.output(x)
            self.weights[1:] += self.learning_rate * (y - prediction) * x
            self.weights[0] += self.learning_rate * (y - prediction)
